\section{Methods}
\label{sec:methods}

After introducing the data side of the problem, this chapter is devoted to consider the model parameters, the inverse problem itself and finally appropriate solution methods in more detail.\\
First of all, the aim is to find the model parameter $\mathbf{m} = (c, \rho, k, N, \lambda) \in \mathbb{R}^6$ which best explain the measured viral load $\mathbf{d}_{obs}$.
Generally, the exact values of the parameters are unknown and therefore, are modelled by probability distributions.
In doing so, the following prior knowledge can be incorporated.
Due to extensive, preceding clinical tests on other HIV patients as well as on animals, the rough order of magnitude of the parameters is known.
These prior values are assumed to be the means around which the parameters symmetrically deviate with a certain variance.
It is $E[\mathbf{m}] = (13, 0.01, 8\cdot 10^{-7}, 100, 10 000)^T$ \cite{ADAMS200510} and $Var[\mathbf{m}] = (2, 0.001, 1\cdot 10^{-8}, 3, 300)^T$.
Further note, that the model parameters are independent of each other and consequently uncorrelated.
Further, from basic physics and biology it can be inferred that the model parameters are always non-negative.
Summarizing these constraints, the prior knowledge about the model parameters is represented by a multi-variant gamma distribution $\mathbf{m} \sim p(\mathbf{m}) = \Gamma (\mathbf{k_{\gamma}}, \mathbf{\theta_{\gamma}})$.
Its inherent advantage is, that is does not accept negative model parameters.
Note, that the shape parameters $k_{\gamma}$ and the scaling parameters $\theta_{\gamma}$ of the distributions have to be subjectively chosen for each parameter of $\mathbf{m}$ such that the distribution is symmetric around the given mean of each model parameter.\\ \\
After representing the prior knowledge in form of probability distributions, the next step can be taken and the inverse problem itself is considered.
First of all, the problem with its six unknown model parameters and the six observations is even-determined.
In chapter \ref{sec:Data}, it has been mentioned that \textit{as few as possible} blood plasma test should be conducted.
Here, the minimal number is set by the necessity to generate at least as many observations as unknown parameters in order to avoid an over- or under-determined problem.\\
Following this, the inverse problem has to be formulated.
Generally, there are three possibilities namely defining the inversion as an optimization problem, a deterministic least-squares problem or a Bayesian inverse problem.
The first option is based on approximating a problem-dependent misfit function. This approach is appropriate for a weakly non-linear forward problem.
Opposed to that, a deterministic least-square problem solves the root-mean square misfit function for a linear forward model.
Since this includes the inversion of possibly singular matrices, one has to incorporate prior knowledge as well as regularization techniques (equivalent to include artificial prior knowledge). 
Last but not least, a Bayesian inverse problem derives the a posterior model distribution $p(\mathbf{m}|\mathbf{d}_{obs})$ from the prior distributions in model and data space with Bayes theorem.
Following this, computationally expensive Monte Carlos Methods are used to sample from the posterior distribution since the latter is generally not given explicitly.
The finally selected model is then for instance the \textit{maximum a posterior model} $\mathbf{m}_{MAP}$.
In a special case, where the forward model is linear and both prior distributions are Gaussian, the probabilistic approach of Bayes can be formulated as a least-squares problem and the posterior distribution can be defined explicitly.\\
However, the fact that the forward model for the HIV dynamics is non-linear and none of the prior distributions is purely Gaussian, directly excludes the least-square approach.
Further, one could either choose to linearize the forward model and define the problem as an optimization or to formulate it as a Bayes inverse problem.
An often mentioned disadvantages of the latter is, that sampling from a distribution with Monte Carlo Methods is computationally expensive.
Apart from that, Bayes approach has no inherent limitation and even has the strong advantage to directly include an uncertainty quantification since the likelihood is directly attached to each model as a measure of plausibility.\\
Hence, despite the high computational intensity it is reasonable to formulate the problem as a Bayes inversion.
It is 

\begin{align}
    p(\mathbf{m}|\mathbf{d}_{obs}) = \frac{p(\mathbf{d}_{obs}|\mathbf{m})p(\mathbf{m})}{p(\mathbf{d}_{obs})} \quad .
\end{align}

As mentioned above, in most cases the posterior model distribution $p(\mathbf{m}|\mathbf{d}_{obs})$ can not be computed analytically and hence Bayes approach has to be equipped with an algorithm to sample from it.
A very simple choice would by the a systematic grid search in the model space.
However, since it heavily suffers from the curse of dimensionality Monte Carlo Methods (MCM) are more appropriate.
These are random search algorithms which try to avoid regions in the model space that are less relevant by taking the prior knowledge into account.
A special MCM algorithm is the so-called Simulated Annealing (SA).
Based on the Metropolis-Hasting acceptance rule, it samples from a modified posterior probability distribution $p_T(\mathbf{m}|\mathbf{d}_{obs})$.
Due to this modification of the distribution, the algorithm starts sampling from a very broad $p_T(\mathbf{m}|\mathbf{d}_{obs})$.
In the course of the SA sampling, the peaks of the modified distribution become more and more distinct.
For a sufficiently slow transition from the broad $p_T(\mathbf{m}|\mathbf{d}_{obs})$ to the distinct one, convergence towards the global maximum-likelihood model is guaranteed.\\
Hence, the solution of the inverse problem is automatically the maximum likelihood model $\mathbf{m}_{MAP}$.
As mentioned earlier, Bayes approach does not require any additional uncertainty assessment.